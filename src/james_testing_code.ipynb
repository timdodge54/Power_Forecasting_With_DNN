{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6130/2832444036.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import skorch\n",
    "import torch\n",
    "from skorch import helper \n",
    "from skorch import NeuralNetRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from skorch.helper import predefined_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pytz\n",
    "utc=pytz.UTC\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "len_in  = 25 # number of half hour steps to make the input\n",
    "len_out = 1  # number of half hour steps to make the input\n",
    "\n",
    "train_data_stop    = utc.localize(datetime.datetime(2013, 6, 1, 0, 0, 0))\n",
    "validate_data_stop = utc.localize(datetime.datetime(2014, 1, 1, 0, 0, 0))\n",
    "\n",
    "data_df = pd.read_csv('../norm_data.csv')\n",
    "data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], utc=True)\n",
    "\n",
    "# Make datasets \n",
    "X_train = []\n",
    "y_train = []\n",
    "X_validate = []\n",
    "y_validate = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for (ind, date) in enumerate(data_df['timestamp'][:-(len_in+len_out)]):\n",
    "    X = np.array(\n",
    "            data_df['avg_energy'][ ind:ind+len_in].append(\n",
    "            data_df['is_holiday'][ ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['visibility'][ ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['temperature'][ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['dewPoint'][   ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['pressure'][   ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['windSpeed'][  ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['precipType'][ ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['humidity'][   ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['hour_minute'][ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['month'][      ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['day'][        ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['year'][       ind:ind+len_in], ignore_index=True).append(\n",
    "            data_df['is_weekday'][ ind:ind+len_in], ignore_index=True),\n",
    "        dtype=float,\n",
    "    )\n",
    "    y = np.array(data_df['avg_energy'][ind+len_in:ind+len_in+len_out], dtype=float)\n",
    "    if date < train_data_stop:\n",
    "        X_train.append(X)\n",
    "        y_train.append(y)\n",
    "    elif date < validate_data_stop:\n",
    "        X_validate.append(X)\n",
    "        y_validate.append(y)\n",
    "    else:\n",
    "        X_test.append(X)\n",
    "        y_test.append(y)\n",
    "        break ######\n",
    "X_train    = torch.from_numpy(np.array(X_train)).   float()\n",
    "y_train    = torch.from_numpy(np.array(y_train)).   float()\n",
    "X_validate = torch.from_numpy(np.array(X_validate)).float()\n",
    "y_validate = torch.from_numpy(np.array(y_validate)).float()\n",
    "X_test     = torch.from_numpy(np.array(X_test)).    float()\n",
    "y_test     = torch.from_numpy(np.array(y_test)).    float()\n",
    "\n",
    "# Define network \n",
    "class FullConnected(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = [nn.Linear(input_dim, hidden_dims[0])]\n",
    "        prev_size = input_dim\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.layers.append(nn.Linear(prev_size+hidden_dims[i], hidden_dims[i+1]))\n",
    "            prev_size += hidden_dims[i]\n",
    "        self.layers.append(nn.Linear(prev_size+hidden_dims[-1], output_dim))\n",
    "        self.classifier = nn.ModuleList(self.layers)\n",
    "        \n",
    "        self.nl = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for (i, l) in enumerate(self.classifier):\n",
    "            if i == len(self.classifier)-1:\n",
    "                x = self.nl(l(x))\n",
    "            else:\n",
    "                x = torch.cat((x, self.nl(l(x))), dim=1)\n",
    "        return x\n",
    "\n",
    "# Train\n",
    "model = NeuralNetRegressor(module = FullConnected, \n",
    "                           module__input_dim = 14*len_in,\n",
    "                           module__hidden_dims = [1000,1000,1000,1000,1000,1000],\n",
    "                           module__output_dim = len_out,\n",
    "                           max_epochs = 100, \n",
    "                           criterion = torch.nn.MSELoss, \n",
    "                           optimizer = torch.optim.SGD,\n",
    "                           lr = 0.1,\n",
    "                           batch_size = 256,\n",
    "                           device = device,\n",
    "                           callbacks=[skorch.callbacks.EarlyStopping(patience=5)], \n",
    "                                      #skorch.callbacks.Checkpoint(monitor='valid_loss_best')],\n",
    "                           train_split=predefined_split(skorch.dataset.Dataset(X_validate, y_validate)),\n",
    "                           verbose = 1,\n",
    "                           iterator_train__num_workers = 2,\n",
    "                           iterator_train__pin_memory = True,\n",
    "                           iterator_valid__num_workers = 2,\n",
    "                           iterator_valid__pin_memory = True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "## Calculate time differences\n",
    "time_diffs = [(data_df['timestamp'][i] - data_df['timestamp'][i - 1]).total_seconds() / 60 / 60 for i in range(1, len(data_df['timestamp']))]\n",
    "## Plotting\n",
    "plt.plot(data_df['timestamp'][1:], time_diffs, marker='o', linestyle='-')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Time Differences (in hour)')\n",
    "plt.title('Time Differences Between Datetime Objects')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/james/classes/Power_Forecasting_With_DNN/src/james_testing_code.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/james/classes/Power_Forecasting_With_DNN/src/james_testing_code.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/james/classes/Power_Forecasting_With_DNN/src/james_testing_code.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/james/classes/Power_Forecasting_With_DNN/src/james_testing_code.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def PrecipTypeToVal(precip_type):\n",
    "    if precip_type == 'rain':\n",
    "        return 0\n",
    "    elif precip_type == 'snow':\n",
    "        return 1\n",
    "    else:\n",
    "        raise RuntimeError('that is not a good precip type')\n",
    "    \n",
    "def ValToPrecipType(precip_type):\n",
    "    if precip_type == 0:\n",
    "        return 'rain'\n",
    "    elif precip_type == 1:\n",
    "        return 'snow'\n",
    "    else:\n",
    "        raise RuntimeError('that is not a good precip type value')\n",
    "\n",
    "def getData(data_dir = '../data/'):\n",
    "    # Load data \n",
    "    holidays_df       = pd.read_csv(data_dir + 'uk_bank_holidays.csv')\n",
    "    weather_hourly_df = pd.read_csv(data_dir + 'weather_hourly_darksky.csv')\n",
    "    by_meter_df       = pd.read_csv(data_dir + 'informations_households.csv')\n",
    "\n",
    "    half_hour_power_df = pd.read_csv(data_dir + \"halfhourly_dataset/halfhourly_dataset/block_0.csv\")\n",
    "    for block_it in range(1,112):\n",
    "        half_hour_power_block = pd.read_csv(data_dir + \"halfhourly_dataset/halfhourly_dataset/block_\" + str(block_it) + \".csv\")\n",
    "        half_hour_power_df = pd.concat([half_hour_power_block], ignore_index=True)\n",
    "    holidays_df = holidays_df.drop('Type', axis=1)\n",
    "    holidays_df['Bank holidays'] = pd.to_datetime(holidays_df['Bank holidays'], format='%Y-%m-%d', utc=True)\n",
    "    weather_hourly_df = weather_hourly_df.rename(columns={\"time\": \"timestamp\"})\n",
    "    weather_hourly_df = weather_hourly_df.drop(['icon', 'windBearing', 'apparentTemperature', 'summary'], axis=1)\n",
    "    weather_hourly_df['timestamp'] = pd.to_datetime(weather_hourly_df['timestamp'], utc=True)\n",
    "    by_meter_df = by_meter_df.drop(['stdorToU', 'Acorn', 'file'], axis=1)\n",
    "    half_hour_power_df = half_hour_power_df.rename(columns={\"tstp\": \"timestamp\"})\n",
    "    half_hour_power_df['timestamp'] = pd.to_datetime(half_hour_power_df['timestamp'], utc=True)\n",
    "    half_hour_power_df = half_hour_power_df[half_hour_power_df['energy(kWh/hh)'] != 'Null']\n",
    "    half_hour_power_df['energy(kWh/hh)'] = half_hour_power_df['energy(kWh/hh)'].astype('float')\n",
    "\n",
    "    # Get time vec\n",
    "    weather_hourly_df = weather_hourly_df.sort_values(by='timestamp')\n",
    "    start_time = weather_hourly_df['timestamp'].iloc[0]\n",
    "    end_time = weather_hourly_df['timestamp'].iloc[-1]\n",
    "    iterated_time = start_time + timedelta(minutes=30)\n",
    "    all_needed_times = [copy.deepcopy(iterated_time)]\n",
    "    while iterated_time < end_time:\n",
    "        iterated_time = iterated_time + timedelta(minutes=30)\n",
    "        all_needed_times.append(copy.deepcopy(iterated_time))\n",
    "    time_df = pd.DataFrame({'timestamp': all_needed_times})\n",
    "\n",
    "    # Interpolate weather data\n",
    "    weather_half_hour_df = pd.merge(time_df, weather_hourly_df, on='timestamp', how='left')\n",
    "    weather_half_hour_df.sort_values(by='timestamp', inplace=True)\n",
    "    weather_half_hour_df['precipType'].fillna(method='ffill', inplace=True)\n",
    "    weather_half_hour_df['precipType'].fillna(method='bfill', inplace=True)\n",
    "    for col_it in ['temperature', 'dewPoint', 'pressure', 'windSpeed', 'humidity', 'visibility']:\n",
    "        weather_half_hour_df[col_it].interpolate(method='quadratic', inplace=True)\n",
    "        weather_half_hour_df[col_it].fillna(method='ffill', inplace=True)\n",
    "        weather_half_hour_df[col_it].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Replace precipType with index values\n",
    "    weather_half_hour_df['precipType'] = weather_half_hour_df['precipType'].apply(lambda x: PrecipTypeToVal(x))\n",
    "\n",
    "    # Add holidays \n",
    "    weather_half_hour_df = weather_half_hour_df.merge(holidays_df, left_on = 'timestamp', right_on = 'Bank holidays', how = 'left')\n",
    "    weather_half_hour_df['Bank holidays'] = np.where(weather_half_hour_df['Bank holidays'].isna(), 0, 1)\n",
    "\n",
    "    # Put it all together\n",
    "    housecount   = half_hour_power_df.groupby('timestamp')[['LCLid']].nunique().sort_values(by='timestamp').astype('float')\n",
    "    total_energy = half_hour_power_df.groupby('timestamp')[['energy(kWh/hh)']].sum().sort_values(by='timestamp').astype('float')\n",
    "\n",
    "    weather_half_hour_df = pd.merge(housecount, weather_half_hour_df, on='timestamp', how='left')\n",
    "    weather_half_hour_df = weather_half_hour_df.rename(columns={\"LCLid\": \"num_houses\"})\n",
    "    \n",
    "    weather_half_hour_df = pd.merge(total_energy, weather_half_hour_df, on='timestamp', how='left')\n",
    "    weather_half_hour_df = weather_half_hour_df.rename(columns={\"energy(kWh/hh)\": \"total_energy\"})\n",
    "    \n",
    "    weather_half_hour_df.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "\n",
    "    output = pd.DataFrame(data={'timestamp': weather_half_hour_df['timestamp'],\n",
    "                                'avg_energy': weather_half_hour_df['total_energy'] / weather_half_hour_df['num_houses'],\n",
    "                                'num_houses': weather_half_hour_df['num_houses'],\n",
    "                                'is_holiday': weather_half_hour_df['Bank holidays'],\n",
    "                                'visibility': weather_half_hour_df['visibility'],\n",
    "                                'temperature': weather_half_hour_df['temperature'],\n",
    "                                'dewPoint': weather_half_hour_df['dewPoint'],\n",
    "                                'pressure': weather_half_hour_df['pressure'],\n",
    "                                'windSpeed': weather_half_hour_df['windSpeed'],\n",
    "                                'precipType': weather_half_hour_df['precipType'],\n",
    "                                'humidity': weather_half_hour_df['humidity']})\n",
    "    output.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Normalize \n",
    "    normalization_cols = [i for i in output.columns.tolist() if i not in ['timestamp', 'precipType', 'num_houses', 'is_holiday']]\n",
    "    normalization_vals = {}\n",
    "    for col_name in normalization_cols:\n",
    "        normalization_vals[col_name] = {}\n",
    "        normalization_vals[col_name]['min'] = output[col_name].min()\n",
    "        normalization_vals[col_name]['max'] = output[col_name].max()\n",
    "        normalization_vals[col_name]['std'] = output[col_name].std()\n",
    "    normalizer = MinMaxScaler(feature_range=(0, 1))\n",
    "    output[normalization_cols] = normalizer.fit_transform(output[normalization_cols])\n",
    "\n",
    "    return output, normalization_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df, _ = getData()\n",
    "df.to_csv(path_or_buf=\"../normalized_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
