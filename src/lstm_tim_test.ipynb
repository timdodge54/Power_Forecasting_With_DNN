{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import datetime\n",
    "\n",
    "utc=pytz.UTC\n",
    "\n",
    "\n",
    "len_in  = 48 # number of half hour steps to make the input\n",
    "len_out = 48  # number of half hour steps to make the input\n",
    "\n",
    "train_data_stop    = utc.localize(datetime.datetime(2013, 5, 1, 0, 0, 0))\n",
    "validate_data_stop = utc.localize(datetime.datetime(2013, 11, 1, 0, 0, 0))\n",
    "\n",
    "data_df = pd.read_csv('../norm_data.csv')\n",
    "data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], utc=True)\n",
    "\n",
    "# Make datasets \n",
    "X_train = []\n",
    "y_train = []\n",
    "X_validate = []\n",
    "y_validate = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for (ind, date) in enumerate(data_df['timestamp'][:-(len_in+len_out)]):\n",
    "    day_X = []\n",
    "\n",
    "    for i in range(ind, ind+len_in):\n",
    "        X = np.array(\n",
    "            [\n",
    "                    data_df['avg_energy'][ind: ind+len_in],\n",
    "                    # data_df['is_holiday'][ind],\n",
    "                    # data_df['visibility'][ind],\n",
    "                    data_df['temperature'][ind: ind+len_in],\n",
    "                    data_df['dewPoint'][ind: ind+len_in],\n",
    "                    # data_df['pressure'][ind],\n",
    "                    # data_df['windSpeed'][ind],\n",
    "                    data_df['precipType'][ind: ind+len_in],\n",
    "                    data_df['humidity'][ind: ind+len_in],\n",
    "                    data_df['hour_minute'][ind: ind+len_in],\n",
    "                    data_df['month'][ind: ind+len_in],\n",
    "                    data_df['day'][ind: ind+len_in],\n",
    "                    # data_df['year'][ind],\n",
    "                    data_df['is_weekday'][ind: ind+len_in],\n",
    "            ],\n",
    "            dtype=float,\n",
    "        )\n",
    "        day_X.append(X)\n",
    "    y = np.array(data_df['avg_energy'][ind+len_in:ind+len_in+len_out], dtype=float)\n",
    "    if date < train_data_stop:\n",
    "        X_train.append(day_X)\n",
    "        y_train.append(y)\n",
    "    elif date < validate_data_stop:\n",
    "        X_validate.append(day_X)\n",
    "        y_validate.append(y)\n",
    "    else:\n",
    "        X_test.append(day_X)\n",
    "        y_test.append(y)\n",
    "X_train, y_train = torch.Tensor(np.array(X_train)), torch.Tensor( np.array(y_train))\n",
    "X_validate, y_validate = torch.Tensor(np.array(X_validate)), torch.Tensor( np.array(y_validate))\n",
    "X_test, y_test = torch.Tensor(np.array(X_test)), torch.Tensor( np.array(y_test))\n",
    "display(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "validate_dataset = torch.utils.data.TensorDataset(X_validate, y_validate)\n",
    "validateloader = torch.utils.data.DataLoader(validate_dataset, batch_size=128, shuffle=False)\n",
    "X_test, y_test = Variable(X_test), Variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, connecting_size, bi_dir=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_directions = 2 if bi_dir else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=bi_dir)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc = nn.Linear(int(hidden_size*self.num_directions), connecting_size)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(connecting_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers*self.num_directions, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers*self.num_directions, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        '''\n",
    "        ***Explain*** Why do we need h_0 and c_0?\n",
    "        '''\n",
    "        \n",
    "        hall, (h_out, c_out) = self.lstm(x, (h_0, c_0))\n",
    "\n",
    "        out = self.fc(hall).squeeze(-1)\n",
    "\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out).squeeze(-1)\n",
    "        # display(out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(9, 48, batch_first=True, bidirectional=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc): Linear(in_features=96, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.12246260576058125\n",
      "Epoch: 100 Loss: 0.14561706025531326\n",
      "Epoch: 200 Loss: 0.14566756644542667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_seq)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, output_seq)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m mean_loss \u001b[39m=\u001b[39m total \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(validateloader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTM(num_classes=y_train.shape[1], input_size=9, hidden_size=48 , num_layers=1, connecting_size=128, bi_dir=True)\n",
    "display(model)\n",
    "criterion = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 5\n",
    "model.train()\n",
    "model.to(device)\n",
    "prv_acc = 0\n",
    "loss_ = []\n",
    "acc_ = []\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    jlast = 0\n",
    "    model.train()\n",
    "    for j, batch in enumerate(trainloader):\n",
    "        input_seq, output_seq = batch\n",
    "        input_seq, output_seq = Variable(input_seq).to(device), Variable(output_seq).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_seq)\n",
    "        loss = criterion(outputs, output_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_.append(loss.item())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0 \n",
    "        total = 0\n",
    "        for j, (input_seq, output_seq) in enumerate(validateloader):\n",
    "            input_seq, output_seq = input_seq.to(device), output_seq.to(device)\n",
    "            input_seq, output_seq = Variable(input_seq).to(device), Variable(output_seq).to(device)\n",
    "            outputs = model(input_seq)\n",
    "            loss = criterion(outputs, output_seq)\n",
    "            total += loss.item()\n",
    "        mean_loss = total / len(validateloader)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {i} Loss: {mean_loss}\")\n",
    "        if np.abs(prv_acc - mean_loss) < .001:\n",
    "            break\n",
    "        prev_acc = mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17280, 49, 14])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacty of 3.61 GiB of which 672.19 MiB is free. Including non-PyTorch memory, this process has 2.93 GiB memory in use. Of the allocated memory 2.57 GiB is allocated by PyTorch, and 236.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_validate_2 \u001b[39m=\u001b[39m X_validate[\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(X_validate)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m):\u001b[39mlen\u001b[39m(X_validate)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m display(X_validate\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m output \u001b[39m=\u001b[39m model(X_validate_2)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output,y_validate)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy of the network is \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/machine_learning/Power_Forecasting_With_DNN/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/machine_learning/Power_Forecasting_With_DNN/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m c_0 \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_directions, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m***Explain*** Why do we need h_0 and c_0?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m hall, (h_out, c_out) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h_0, c_0))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# display(f\"After LSTM: {hall.shape}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tim/code/machine_learning/Power_Forecasting_With_DNN/src/lstm_tim_test.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m hall \u001b[39m=\u001b[39m hall\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/code/machine_learning/Power_Forecasting_With_DNN/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/machine_learning/Power_Forecasting_With_DNN/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/machine_learning/Power_Forecasting_With_DNN/venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 792.00 MiB. GPU 0 has a total capacty of 3.61 GiB of which 672.19 MiB is free. Including non-PyTorch memory, this process has 2.93 GiB memory in use. Of the allocated memory 2.57 GiB is allocated by PyTorch, and 236.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# get the last half of the X_validate\n",
    "output = model(X_test)\n",
    "loss = criterion(output,y_test)\n",
    "print(f\"Accuracy of the network is {loss.item()}\")\n",
    "last48 = output.detach().numpy()[-48:]\n",
    "plt.plot(last48, label='pred')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
